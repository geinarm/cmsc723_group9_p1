Team: 9
1- Gudjon Magnusson
2- Irina Yakubinskaya
3-

Email: 
gudjon@terpmail.umd.edu
irinya@terpmail.umd.edu
---------------------------------------------------------------------------------
Part 1)

1- baseline accuracy = 55.54% (for dev set) 
2- Cohenâ€™s Kappa = 0.796  #17 examples were annotated correctly

---------------------------------------------------------------------------------
Part 2)
1- 


s   | cord | division   | formation | phone | product | text
-----------------------------------------------------------------
c(s)|  273 |    253     |     251   |  312  |  1526   |   287


s        | cord | division   | formation | phone | product | text
-----------------------------------------------------------------
c(s,time)|  12  |    13     |    16     |  15    |    43    |  19

s        | cord | division   | formation | phone | product | text
-----------------------------------------------------------------
c(s,loss)|   1  |    0       |    0      |  2    |    23    |    0

s          | cord | division   | formation | phone | product | text
---------------------------------------------------------------------
c(s,export)|   0  |    0       |    0      |  1    |    3    |    0



2-

s   | cord | division   | formation | phone | product | text
-----------------------------------------------------------------
p(s)| 0.0941 | 0.0872   |   0.0865  | 0.1075 | 0.5258 |  0.0989

s        | cord | division   | formation | phone | product | text
-----------------------------------------------------------------
p(s|time)| 0.1048 |  0.1129  |   0.1370   | 0.1290| 0.3548  | 0.1612

s        | cord | division   | formation | phone | product | text
-----------------------------------------------------------------
p(s|loss)|  0.0625 | 0.03125 |  0.03125  | 0.0937 |  0.75  |  0.03135


s          | cord | division   | formation | phone | product | text
---------------------------------------------------------------------
p(s|export)| 0.1  |   0.1      |    0.1     |  0.2 |   0.4   |  0.1


3- for the sentence X = "and i can tell you that i 'm an absolute nervous wreck every time she performs . i have her practice the last two lines on each page , so I can learn exactly when to turn the page -- just one of the tricks to this trade that i 've learned the hard way ."

s     | cord | division   | formation | phone | product | text
-----------------------------------------------------------------
p(s|X)| 3.3251e-146 |  2.70713e-148 | 2.45e-147 | 1.6600e-146 | 2.399e-149 | 6.3023e-143


4- classifier f-measures on the test set:
micro averaged = 0.804
macro averaged = 0.701

5- 
The feature used is a binary vector with length equal to the size of the vocabulary. The ith entry is one if w_i is present in the sample X, zero otherwise.
The probability p(s|w) is calculated by counting occurrences of words in that class. To avoid division by zero and smooth the results we add one to all counts.
For each class we create a weight vector with length equal to the vocabulary plus one. The last entry is the prior for that class. All probabilities are transformed into log-space.
To classify a new example we first generate the feature vector and the app end a one at the end so the length is |V|+1. We take the dot product of the feature vector with each of the weight vectors. The prediction is the class that corresponds to the weight vector that gives the highest response.
---------------------------------------------------------------------------------
Part 3)

1- ._cord: -1.0, ._text: 1.0, ``_cord: -1.0, ``_text: 1.0, her_cord: -2.0, her_text: 2.0, with_cord: -1.0, with_text: 1.0, plucky_cord: -1.0, plucky_text: 1.0, an_cord: -1.0, an_text: 1.0, to_cord: -2.0, to_text: 2.0, plank_cord: -1.0, plank_text: 1.0, jean-jacques_cord: -1.0, jean-jacques_text: 1.0, last_cord: -1.0, last_text: 1.0, by_cord: -1.0, by_text: 1.0, line_cord: -1.0, line_text: 1.0, tied_cord: -1.0, tied_text: 1.0, painting_cord: -1.0, painting_text: 1.0, sits_cord: -1.0, sits_text: 1.0, drawing_cord: -1.0, drawing_text: 1.0, another_cord: -1.0, another_text: 1.0, the_cord: -2.0, the_text: 2.0, much-quoted_cord: -1.0, much-quoted_text: 1.0, not_cord: -1.0, not_text: 1.0, shows_cord: -1.0, shows_text: 1.0, friend_cord: -1.0, friend_text: 1.0, lady_cord: -1.0, lady_text: 1.0, a_cord: -1.0, a_text: 1.0, madame_cord: -1.0, madame_text: 1.0, room_cord: -1.0, room_text: 1.0, before_cord: -1.0, before_text: 1.0, ,_cord: -3.0, ,_text: 3.0, dog_cord: -1.0, dog_text: 1.0, in_cord: -1.0, in_text: 1.0, down_cord: -1.0, down_text: 1.0, managed_cord: -1.0, managed_text: 1.0, did_cord: -1.0, did_text: 1.0, french_cord: -1.0, french_text: 1.0, little_cord: -1.0, little_text: 1.0, rolland_cord: -1.0, rolland_text: 1.0, !_cord: -1.0, !_text: 1.0, exquisite_cord: -1.0, exquisite_text: 1.0, harp_cord: -1.0, harp_text: 1.0, pet_cord: -1.0, pet_text: 1.0, ah_cord: -1.0, ah_text: 1.0, liberty_cord: -1.0, liberty_text: 1.0, she_cord: -1.0, she_text: 1.0, and_cord: -1.0, and_text: 1.0, of_cord: -1.0, of_text: 1.0, hauer_cord: -1.0, hauer_text: 1.0, lafayette_cord: -1.0, lafayette_text: 1.0, who_cord: -1.0, who_text: 1.0, wordsworth_cord: -1.0, wordsworth_text: 1.0

2- comma separated accuracies (e.g. 30,35,60):  0.8949, 0.9304, 0.9721

3- classifier f-measures on the test set:
micro averaged = 0.8347
macro averaged = 0.7408

4- 
The feature used is a binary vector with length equal to the size of the vocabulary plus bias set to 1. The ith entry counts the number of times word_i is present in the sample X, zero otherwise. 
For each class (sense of word to be disambiguated) we create a weight vector with length equal to the vocabulary plus one. All values set to zero.
We build our prediction for class by taking the maximum out of dot products between feature vector and each weight vector. We compare prediction with golden label and if prediction is correct, we do nothing. If prediction is wrond we update weight vector. At each iteration we add new weight vector to the weight vector at previous step. It will be used for averaging.
At the end of training the final weight is calculated by averaging over all update steps.
---------------------------------------------------------------------------------
Part 4)
A) Feature A:

1- Description
This feature is based on BOW but the dimensions are reduced by using principal component analysis.
The roughly 15k dimensions are projected down to a 500 dimension feature space. This is implemented using singular value decomposition.
Unfortunately this takes a very long time to compute and the results are not impressive.
Another problem with this feature is that it removes any notion of counts or probabilities. The features are just coordinates in some abstract space. This does not work well for naive bayes.

2- naive-bayes f-measures on the test set:
micro averaged = 0.08026
macro averaged = 0.02476

3- perceptron f-measures on the test set:
micro averaged = 0.7929
macro averaged = 0.6682


4- Conclusions:
With more time to tweak and a faster implementation this might be a good feature to use for the perception, but does not seam like a good choice for naive beyes.

B) Feature B:

1- Description
The idea for this one was to use the count of word overlap with the dictionary definition of each of the senses. 
Definitions were gathered from the dictionary provided by Google. The definitions includes descriptions, synonyms and examples.
At first we used only 6 features, the total number of words that occurred in the example and each definition. We also tried scaling the weight of each word by how common it was. This did work very well.
In the end we used a BOW feature but with a reduced vocabulary with only words in the definition, 385 words in total.
Its not as good using the full vocabulary, but its extremely fast.

2- naive-bayes f-measures on the test set:
micro averaged = 0.7191011235955056
macro averaged = 0.54655630081013795

3- perceptron f-measures on the test set:
micro averaged = 0.7143
macro averaged = 0.5535

4- Conclusions:
This implementation did not produce a very good feature but with some smart tweaking it might be good.
The speed is nice.


