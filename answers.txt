Team: 9
1- Gudjon Magnusson
2- Irina Yakubinskaya
3-

Email: 
gudjon@terpmail.umd.edu
irinya@terpmail.umd.edu
---------------------------------------------------------------------------------
Part 1)

1- baseline accuracy = 55.54% (for dev set) 
2- Cohenâ€™s Kappa = 0.796  #17 examples were annotated correctly

---------------------------------------------------------------------------------
Part 2)
1- 


s   | cord | division   | formation | phone | product | text
-----------------------------------------------------------------
c(s)|  273 |    253     |     251   |  312  |  1526   |   287


s        | cord | division   | formation | phone | product | text
-----------------------------------------------------------------
c(s,time)|  12  |    13     |    16     |  15    |    43    |  19

s        | cord | division   | formation | phone | product | text
-----------------------------------------------------------------
c(s,loss)|   1  |    0       |    0      |  2    |    23    |    0

s          | cord | division   | formation | phone | product | text
---------------------------------------------------------------------
c(s,export)|   0  |    0       |    0      |  1    |    3    |    0



2-

s   | cord | division   | formation | phone | product | text
-----------------------------------------------------------------
p(s)| 0.0941 | 0.0872   |   0.0865  | 0.1075 | 0.5258 |  0.0989

s        | cord | division   | formation | phone | product | text
-----------------------------------------------------------------
p(s|time)| 0.1048 |  0.1129  |   0.1370   | 0.1290| 0.3548  | 0.1612

s        | cord | division   | formation | phone | product | text
-----------------------------------------------------------------
p(s|loss)|  0.0625 | 0.03125 |  0.03125  | 0.0937 |  0.75  |  0.03135


s          | cord | division   | formation | phone | product | text
---------------------------------------------------------------------
p(s|export)| 0.1  |   0.1      |    0.1     |  0.2 |   0.4   |  0.1


3- for the sentence X = "and i can tell you that i 'm an absolute nervous wreck every time she performs . i have her practice the last two lines on each page , so I can learn exactly when to turn the page -- just one of the tricks to this trade that i 've learned the hard way ."

s     | cord | division   | formation | phone | product | text
-----------------------------------------------------------------
p(s|X)| 3.3251e-146 |  2.70713e-148 | 2.45e-147 | 1.6600e-146 | 2.399e-149 | 6.3023e-143


4- classifier f-measures on the test set:
micro averaged = 0.804
macro averaged = 0.701

5- 
The feature used is a binary vector with length equal to the size of the vocabulary. The ith entry is one if w_i is present in the sample X, zero otherwise.
The probability p(s|w) is calculated by counting occurrences of words in that class. To avoid division by zero and smooth the results we add one to all counts.
For each class we create a weight vector with length equal to the vocabulary plus one. The last entry is the prior for that class. All probabilities are transformed into log-space.
To classify a new example we first generate the feature vector and the app end a one at the end so the length is |V|+1. We take the dot product of the feature vector with each of the weight vectors. The prediction is the class that corresponds to the weight vector that gives the highest response.
---------------------------------------------------------------------------------
Part 3)

1- 
W_text
.:1, ``:1, her:1, with:1, plucky':1, an:1, to:1, plank:1, jacques:1, last:1, by:1, line:1, tied:1, painting:1, sits:1, drawing:1, another:1, the:1, quoted:1, not:1, shows:1, friend:1, lady:1, a:1, madame:1, room:1, before:1, ,:1, dog:1, in:1, down:1, managed:1, did:1, french:1, little:1, rolland:1, !:1, exquisite:1, harp:1, pet:1, ah:1, liberty:1, she:1, and:1, of:1, hauer:1, lafayette:1, who:1, wordsworth:1
W_cord
.:-1, ``:-1, her:-1, with:-1, plucky':-1, an:-1, to:-1, plank:-1, jacques:-1, last:-1, by:-1, line:-1, tied:-1, painting:-1, sits:-1, drawing:-1, another:-1, the:-1, quoted:-1, not:-1, shows:-1, friend:-1, lady:-1, a:-1, madame:-1, room:-1, before:-1, ,:-1, dog:-1, in:-1, down:-1, managed:-1, did:-1, french:-1, little:-1, rolland:-1, !:-1, exquisite:-1, harp:-1, pet:-1, ah:-1, liberty:-1, she:-1, and:-1, of:-1, hauer:-1, lafayette:-1, who:-1, wordsworth:-1

2- comma separated accuracies (e.g. 30,35,60): 
0.8949, 0.9304, 0.9721

3- classifier f-measures on the test set:
micro averaged = 0.8347
macro averaged = 0.7408
4- 
---------------------------------------------------------------------------------
Part 4)
A) Feature A:

1- Description

2- naive-bayes f-measures on the test set:
micro averaged = 
macro averaged = 

3- perceptron f-measures on the test set:
micro averaged = 
macro averaged = 


4- Conclusions:

B) Feature B:

1- Description

2- naive-bayes f-measures on the test set:
micro averaged = 
macro averaged = 

3- perceptron f-measures on the test set:
micro averaged = 
macro averaged = 


4- Conclusions:


